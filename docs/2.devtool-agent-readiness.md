# DevTool Agent Readiness Evaluator
## Project Specification v2.0 (RAG-Enhanced)

## System Overview (Updated)

### Enhanced Architecture with RAG

```
Documentation Source (URL/GitHub)
        ↓
   CONTENT FETCHING
   (Website/GitHub/OpenAPI)
        ↓
   VECTOR STORE CREATION
   (Semantic Documentation Index)
        ↓
    PLANNER AGENT
   (RAG-Enhanced Use Case Generation)
        ↓
    CODING AGENT (RAG + Self-Correction Loop)
   (Implement → Test → Analyze → Fix)
        ↓
   EVALUATOR AGENT
   (Documentation Quality Assessment)
        ↓
   Readiness Report + Doc Improvement Suggestions
```

## Enhanced State Management

### Updated Pydantic Models

```python
from pydantic import BaseModel, Field
from typing import Dict, List, Optional, Any, Union
from enum import Enum
from datetime import datetime

class DocumentationChunk(BaseModel):
    """Represents a semantically meaningful chunk of documentation"""
    chunk_id: str = Field(..., description="Unique identifier for this chunk")
    content: str = Field(..., description="The actual documentation content")
    source_section: str = Field(..., description="Section/page this came from")
    chunk_type: str = Field(..., description="code_example, explanation, api_reference, etc.")
    embedding_vector: Optional[List[float]] = Field(None, description="Vector embedding of content")

class RAGContext(BaseModel):
    """Context retrieved from vector store for a specific query"""
    query: str = Field(..., description="The query that was used for retrieval")
    retrieved_chunks: List[DocumentationChunk] = Field(..., description="Relevant documentation chunks")
    relevance_scores: List[float] = Field(..., description="Relevance scores for each chunk")
    total_chunks_available: int = Field(..., description="Total chunks in vector store")

class CodeAttempt(BaseModel):
    """Single attempt at implementing code for a use case"""
    attempt_number: int = Field(..., description="Which attempt this is (1, 2, 3...)")
    timestamp: datetime = Field(default_factory=datetime.now)
    
    # RAG Information
    rag_context: RAGContext = Field(..., description="Documentation retrieved for this attempt")
    
    # Generated Code
    prefix: str = Field(..., description="Description of the approach")
    imports: str = Field(..., description="Import statements")
    code: str = Field(..., description="Main code block")
    
    # Execution Results
    import_test_passed: bool = Field(default=False)
    import_error: Optional[str] = Field(None)
    execution_test_passed: bool = Field(default=False)
    execution_error: Optional[str] = Field(None)
    execution_output: Optional[str] = Field(None)
    
    # Documentation Analysis
    documentation_helpful: bool = Field(..., description="Was retrieved documentation useful?")
    missing_information: List[str] = Field(default_factory=list, description="What info was missing from docs")
    confusing_parts: List[str] = Field(default_factory=list, description="What parts of docs were unclear")
    suggested_improvements: List[str] = Field(default_factory=list, description="How to improve the docs")

class UseCaseExecution(BaseModel):
    """Complete execution record for one use case with iterative attempts"""
    use_case_id: str = Field(..., description="Reference to the UseCase")
    status: TaskStatus = Field(default=TaskStatus.PLANNED)
    start_time: Optional[datetime] = Field(None)
    end_time: Optional[datetime] = Field(None)
    
    # Multiple attempts with RAG and self-correction
    attempts: List[CodeAttempt] = Field(default_factory=list)
    final_working_code: Optional[str] = Field(None, description="Final successful implementation")
    
    # Success Analysis
    success_criteria_met: Dict[str, bool] = Field(default_factory=dict)
    overall_success: bool = Field(default=False)
    attempts_needed: int = Field(default=0, description="How many attempts were required")
    
    # Documentation Quality Insights
    documentation_quality_score: Optional[float] = Field(None, ge=0, le=10)
    key_documentation_gaps: List[str] = Field(default_factory=list)
    documentation_strengths: List[str] = Field(default_factory=list)

class VectorStoreMetadata(BaseModel):
    """Information about the created vector store"""
    total_chunks: int = Field(..., description="Total number of documentation chunks")
    embedding_model: str = Field(..., description="Model used for embeddings")
    chunk_strategy: str = Field(..., description="How documentation was chunked")
    average_chunk_size: int = Field(..., description="Average characters per chunk")
    coverage_analysis: Dict[str, int] = Field(..., description="Types of content and their counts")

class AgentReadinessState(BaseModel):
    """Enhanced shared state with RAG capabilities"""
    
    # Input Information
    tool_name: str = Field(..., description="Name of the tool being evaluated")
    documentation_source: str = Field(..., description="URL or source of documentation")
    tool_category: str = Field(..., description="API, Framework, Library, Platform, etc.")
    
    # Vector Store Information
    vector_store_metadata: Optional[VectorStoreMetadata] = Field(None)
    vector_store_ready: bool = Field(default=False)
    
    # Enhanced Planner Agent Output
    use_cases: List[UseCase] = Field(default_factory=list)
    planning_rag_context: Optional[RAGContext] = Field(None, description="RAG context used for planning")
    planning_notes: str = Field(default="", description="Planner's analysis and reasoning")
    planning_completed: bool = Field(default=False)
    
    # Enhanced Coding Agent Output  
    executions: List[UseCaseExecution] = Field(default_factory=list)
    environment_setup: Dict[str, Any] = Field(default_factory=dict)
    coding_completed: bool = Field(default=False)
    
    # Enhanced Evaluator Agent Output
    dimension_scores: List[EvaluationDimension] = Field(default_factory=list)
    overall_score: Optional[float] = Field(None, ge=0, le=10)
    documentation_improvement_plan: List[str] = Field(default_factory=list)
    key_insights: List[str] = Field(default_factory=list)
    recommendations: List[str] = Field(default_factory=list)
    evaluation_completed: bool = Field(default=False)
    
    # Meta Information
    evaluation_id: str = Field(..., description="Unique identifier for this evaluation")
    start_time: datetime = Field(default_factory=datetime.now)
    end_time: Optional[datetime] = Field(None)
    total_duration_minutes: Optional[float] = Field(None)
```

## Enhanced Agent Specifications

### 0. Vector Store Creation Component

#### Purpose
Create a semantic index of the documentation that enables precise retrieval of relevant information for each coding task.

#### Key Responsibilities

**Documentation Processing**
- Parse documentation into semantically meaningful chunks
- Identify different content types (examples, API reference, tutorials, troubleshooting)
- Create embeddings using state-of-the-art models
- Build retrieval index optimized for coding queries

**Quality Analysis**
- Assess documentation completeness across different categories
- Identify gaps in code examples vs. explanations
- Analyze the quality and diversity of examples provided

```python
class DocumentationProcessor:
    def create_vector_store(self, documentation_source: str) -> VectorStoreMetadata:
        """
        Process documentation into a searchable vector store
        
        Steps:
        1. Load and parse documentation from source
        2. Chunk into semantic units (preserve code examples intact)
        3. Classify chunks by type (example, explanation, reference, etc.)
        4. Generate embeddings using latest embedding models
        5. Create retrieval index with metadata filtering
        6. Analyze coverage and quality metrics
        """
        pass
```

### 1. Enhanced Planner Agent

#### Updated Objectives
- **RAG-Enhanced Analysis**: Use vector search to understand documentation scope and capabilities
- **Context-Aware Planning**: Generate use cases based on what's actually documented with examples
- **Gap-Aware Design**: Identify areas where documentation seems thin and design targeted tests

#### Enhanced Workflow

```python
def enhanced_planning_process(state: AgentReadinessState) -> AgentReadinessState:
    """
    1. Query vector store for documentation overview
    2. Identify key capabilities and workflows
    3. Search for specific examples and patterns
    4. Generate use cases that test documented vs. undocumented scenarios
    5. Prioritize based on documentation quality and completeness
    """
    
    # Use RAG to understand tool capabilities
    capabilities_context = vector_store.similarity_search(
        "main features core functionality what can this tool do", k=10
    )
    
    # Search for authentication and setup patterns
    auth_context = vector_store.similarity_search(
        "authentication setup installation configuration", k=5
    )
    
    # Look for code examples and workflows  
    examples_context = vector_store.similarity_search(
        "example code sample tutorial getting started", k=8
    )
    
    # Generate use cases based on retrieved documentation
    use_cases = generate_contextual_use_cases(
        capabilities_context, auth_context, examples_context
    )
    
    return state
```

### 2. RAG-Enhanced Coding Agent

#### Primary Goal
Implement solutions using iterative RAG retrieval and self-correction, while analyzing documentation quality at each step.

#### Enhanced Architecture

```python
class RAGCodingAgent:
    def __init__(self, vector_store, max_attempts=3):
        self.vector_store = vector_store
        self.max_attempts = max_attempts
        self.code_gen_chain = self.create_rag_chain()
    
    def execute_use_case(self, use_case: UseCase, state: AgentReadinessState) -> UseCaseExecution:
        """
        Implement use case with RAG + self-correction loop
        
        Flow:
        1. RAG Query: Retrieve relevant documentation
        2. Code Generation: Generate implementation attempt
        3. Code Testing: Test imports and execution
        4. Error Analysis: Analyze what went wrong and why
        5. Documentation Feedback: Assess if docs were helpful
        6. Retry: If failed, refine RAG query and try again
        """
        execution = UseCaseExecution(use_case_id=use_case.id)
        
        for attempt_num in range(1, self.max_attempts + 1):
            attempt = self.make_code_attempt(use_case, execution, attempt_num)
            execution.attempts.append(attempt)
            
            if attempt.import_test_passed and attempt.execution_test_passed:
                execution.overall_success = True
                execution.final_working_code = f"{attempt.imports}\n{attempt.code}"
                break
            else:
                # Analyze what went wrong and prepare for next attempt
                self.analyze_failure_and_prepare_retry(attempt, use_case)
        
        execution.attempts_needed = len(execution.attempts)
        return execution
    
    def make_code_attempt(self, use_case: UseCase, execution: UseCaseExecution, attempt_num: int) -> CodeAttempt:
        """Single attempt with RAG retrieval and documentation analysis"""
        
        # Build RAG query based on use case and previous failures
        rag_query = self.build_rag_query(use_case, execution, attempt_num)
        
        # Retrieve relevant documentation
        rag_context = self.retrieve_documentation(rag_query)
        
        # Generate code using RAG context
        code_solution = self.generate_code(use_case, rag_context, execution)
        
        # Test the generated code
        import_result, execution_result = self.test_code(
            code_solution.imports, code_solution.code
        )
        
        # Analyze documentation quality for this attempt
        doc_analysis = self.analyze_documentation_quality(
            rag_context, code_solution, import_result, execution_result
        )
        
        return CodeAttempt(
            attempt_number=attempt_num,
            rag_context=rag_context,
            prefix=code_solution.prefix,
            imports=code_solution.imports,
            code=code_solution.code,
            import_test_passed=import_result.success,
            import_error=import_result.error,
            execution_test_passed=execution_result.success,
            execution_error=execution_result.error,
            execution_output=execution_result.output,
            documentation_helpful=doc_analysis.helpful,
            missing_information=doc_analysis.missing_info,
            confusing_parts=doc_analysis.confusing_parts,
            suggested_improvements=doc_analysis.suggestions
        )
    
    def build_rag_query(self, use_case: UseCase, execution: UseCaseExecution, attempt_num: int) -> str:
        """Build increasingly specific RAG queries based on previous failures"""
        
        base_query = f"{use_case.description} {use_case.category}"
        
        if attempt_num == 1:
            # First attempt: broad query
            return f"{base_query} example code sample tutorial"
        else:
            # Later attempts: incorporate failure information
            previous_attempt = execution.attempts[-1]
            if previous_attempt.import_error:
                return f"{base_query} imports dependencies installation setup {previous_attempt.import_error}"
            elif previous_attempt.execution_error:
                return f"{base_query} error handling debugging {previous_attempt.execution_error}"
        
        return base_query
    
    def analyze_documentation_quality(self, rag_context: RAGContext, code_solution, import_result, execution_result) -> DocAnalysis:
        """Analyze whether the documentation was helpful and how to improve it"""
        
        analysis_prompt = f"""
        Analyze the documentation quality for this coding task:
        
        Use Case: {code_solution.prefix}
        Retrieved Documentation: {rag_context.retrieved_chunks}
        Generated Code: {code_solution.imports}\n{code_solution.code}
        Import Test: {'PASSED' if import_result.success else f'FAILED: {import_result.error}'}
        Execution Test: {'PASSED' if execution_result.success else f'FAILED: {execution_result.error}'}
        
        Answer these questions:
        1. Was the retrieved documentation helpful for implementing this task?
        2. What specific information was missing from the documentation?
        3. What parts of the documentation were confusing or unclear?
        4. How could the documentation be improved to make this task easier?
        """
        
        # Use LLM to analyze documentation quality
        return self.documentation_analyzer.invoke(analysis_prompt)
```

#### Self-Correction Loop

```python
def coding_agent_workflow():
    """LangGraph workflow for iterative coding with RAG"""
    
    workflow = StateGraph(AgentReadinessState)
    
    # Nodes
    workflow.add_node("rag_query", build_rag_query_node)
    workflow.add_node("generate_code", generate_code_node)
    workflow.add_node("test_code", test_code_node)
    workflow.add_node("analyze_docs", analyze_documentation_node)
    workflow.add_node("prepare_retry", prepare_retry_node)
    
    # Flow
    workflow.add_edge("rag_query", "generate_code")
    workflow.add_edge("generate_code", "test_code")
    workflow.add_edge("test_code", "analyze_docs")
    
    workflow.add_conditional_edges(
        "analyze_docs",
        decide_next_action,
        {
            "success": END,
            "retry": "prepare_retry",
            "max_attempts": END
        }
    )
    
    workflow.add_edge("prepare_retry", "rag_query")
    
    return workflow.compile()
```

### 3. Enhanced Evaluator Agent

#### New Responsibilities

**Documentation Quality Assessment**
- Analyze patterns across all coding attempts
- Identify systematic documentation gaps
- Score documentation helpfulness and clarity
- Generate specific improvement recommendations

**RAG Effectiveness Analysis**
- Evaluate how well vector retrieval worked
- Identify cases where relevant information wasn't retrieved
- Assess chunk quality and coverage

```python
class EnhancedEvaluator:
    def evaluate_documentation_quality(self, state: AgentReadinessState) -> List[EvaluationDimension]:
        """
        Evaluate documentation across multiple dimensions using evidence from coding attempts
        """
        
        dimensions = []
        
        # RAG Retrieval Quality
        dimensions.append(self.evaluate_rag_effectiveness(state))
        
        # Code Example Quality  
        dimensions.append(self.evaluate_code_examples(state))
        
        # Setup/Installation Clarity
        dimensions.append(self.evaluate_setup_clarity(state))
        
        # Error Handling Documentation
        dimensions.append(self.evaluate_error_documentation(state))
        
        # Agent Implementation Success
        dimensions.append(self.evaluate_agent_success_rate(state))
        
        return dimensions
    
    def generate_improvement_plan(self, state: AgentReadinessState) -> List[str]:
        """Generate specific, actionable documentation improvements"""
        
        improvements = []
        
        # Aggregate missing information across all attempts
        all_missing = []
        for execution in state.executions:
            for attempt in execution.attempts:
                all_missing.extend(attempt.missing_information)
        
        # Find most common gaps
        common_gaps = self.find_common_patterns(all_missing)
        
        for gap in common_gaps:
            improvements.append(f"Add documentation section covering: {gap}")
        
        # Analyze retrieval failures
        poor_retrievals = self.find_poor_rag_results(state)
        for failure in poor_retrievals:
            improvements.append(f"Improve searchability by adding keywords: {failure.missing_keywords}")
        
        return improvements
```

## Implementation Plan (Updated)

### Phase 1: RAG Foundation (Weeks 1-4)

**Week 1-2: Vector Store Infrastructure**
- Document chunking and embedding pipeline
- Vector store creation and retrieval system
- Basic RAG chain for code generation
- Documentation quality analysis framework

**Week 3-4: Enhanced Coding Agent**
- Implement iterative coding loop with self-correction
- Add documentation feedback analysis at each attempt
- Build retry mechanism with refined RAG queries
- Test with simple API documentation

### Phase 2: Intelligence Enhancement (Weeks 5-8)

**Week 5-6: Advanced RAG Strategies**
- Implement hybrid retrieval (semantic + keyword)
- Add query expansion and refinement
- Build documentation gap detection
- Enhanced error analysis and recovery

**Week 7-8: Evaluator Intelligence**
- Multi-dimensional documentation scoring
- Pattern recognition across coding attempts
- Automated improvement recommendation generation
- Comparative analysis framework

### Phase 3: Production & Learning (Weeks 9-12)

**Week 9-10: System Integration**
- End-to-end pipeline with all components
- Production vector store infrastructure
- Real-time documentation quality feedback
- User interface for reports and insights

**Week 11-12: Continuous Learning**
- Learning from success/failure patterns
- Documentation improvement tracking
- A/B testing different RAG strategies
- Customer validation and iteration

---

This RAG-enhanced approach provides much richer insights into documentation quality by actually testing whether AI agents can successfully implement solutions using the docs, while providing specific feedback on how to improve them.