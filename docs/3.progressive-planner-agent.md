# Progressive Planner Agent Documentation

## Overview

The Progressive Planner Agent is a sophisticated AI system that generates comprehensive project plans for testing agent readiness with developer tools. Unlike traditional planners that attempt to generate plans immediately, this agent uses a **three-stage progressive approach** that mirrors how experienced developers approach new tools.

## Why Progressive Planning?

### The Problem with Traditional Planners

Most planning systems try to generate use cases without truly understanding the tool:
- **Shallow understanding**: Based on tool name and basic description only
- **Generic plans**: One-size-fits-all approaches that miss tool-specific nuances
- **Poor documentation awareness**: No knowledge of what's actually documented vs. missing
- **No evidence gathering**: Plans aren't grounded in actual documentation quality

### The Progressive Approach

Our planner **learns before planning**, using these principles:
1. **Progressive Exploration**: Build deep understanding through targeted queries
2. **Evidence-Based Planning**: Generate plans based on what's actually documented
3. **Targeted Refinement**: Strengthen each plan with specific supporting evidence
4. **Gap Awareness**: Explicitly identify documentation weaknesses

## Architecture Overview

```
┌─────────────────────────────────────────────────────────────┐
│                    PROGRESSIVE PLANNER                      │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────┐  │
│  │  STAGE 1:       │  │  STAGE 2:       │  │  STAGE 3:   │  │
│  │  Progressive    │  │  Evidence-Based │  │  Targeted   │  │
│  │  Exploration    │  │  Plan Generation│  │  Refinement │  │
│  │                 │  │                 │  │             │  │
│  │ • Query rounds  │  │ • 6 diverse     │  │ • Evidence  │  │
│  │ • Understanding │  │   categories    │  │   gathering │  │
│  │ • Gap detection │  │ • Knowledge-    │  │ • Quality   │  │
│  │ • Confidence    │  │   grounded      │  │   scoring   │  │
│  │   scoring       │  │ • Challenge     │  │ • Difficulty│  │
│  │                 │  │   prediction    │  │   assessment│  │
│  └─────────────────┘  └─────────────────┘  └─────────────┘  │
│           │                     │                     │      │
│           ▼                     ▼                     ▼      │
│  ┌─────────────────────────────────────────────────────────┐  │
│  │              PROJECT SCRATCHPAD                        │  │
│  │  • Cumulative understanding  • Documentation quality   │  │
│  │  • Capability mapping        • Confidence areas       │  │
│  │  • Pattern identification    • Gap identification     │  │
│  └─────────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────┘
```

## Stage 1: Progressive Exploration

### Objective
Build comprehensive understanding of the tool through multiple rounds of targeted documentation queries.

### Process

#### Round 1: Core Functionality
**Query**: `"what is {tool_name} main purpose core functionality overview"`
- **Goal**: Understand what the tool fundamentally does
- **Extracts**: Primary use cases, core concepts, main benefits

#### Round 2: Getting Started
**Query**: `"{tool_name} installation setup getting started quick start tutorial"`
- **Goal**: Understand setup complexity and initial requirements
- **Extracts**: Installation steps, prerequisites, quick start examples

#### Round 3: Common Patterns
**Query**: `"{tool_name} common use cases examples basic usage patterns"`
- **Goal**: Identify typical usage patterns and workflows
- **Extracts**: Common implementations, basic patterns, usage examples

#### Round 4: Authentication & Security
**Query**: `"{tool_name} authentication configuration security setup"`
- **Goal**: Understand security model and configuration requirements
- **Extracts**: Auth methods, security patterns, configuration options

#### Round 5: Advanced Features
**Query**: `"{tool_name} advanced features best practices optimization troubleshooting"`
- **Goal**: Discover advanced capabilities and best practices
- **Extracts**: Advanced features, optimization tips, troubleshooting info

### Analysis Per Round

For each exploration round, the agent:

1. **Retrieves Documentation**: Uses vector similarity search (k=8 documents)
2. **Structured Analysis**: Uses LangChain structured output to ensure reliable parsing
3. **Extracts Key Findings**: Identifies important concepts, examples, and patterns
4. **Categorizes Insights**: Automatically categorizes findings into structured categories:
   - `setup`: Installation, configuration, getting started
   - `authentication`: Security, auth methods, API keys  
   - `core_features`: Main functionality, primary use cases
   - `advanced`: Complex features, customization, optimization
   - `integration`: Connecting with other systems/tools
   - `patterns`: Common usage patterns, best practices
   - `error_handling`: Error codes, troubleshooting, debugging
5. **Assesses Confidence**: Scores documentation quality (0.0-1.0) based on:
   - Number of relevant results found
   - Presence of code examples
   - Diversity of content types (tutorial, API reference, examples)
6. **Identifies Gaps**: Notes areas with poor documentation coverage
7. **Updates Scratchpad**: Accumulates insights for next stages with proper categorization

### Structured Output Model

```python
class StructuredAnalysis(BaseModel):
    key_findings: List[str] = Field(description="Key insights discovered")
    confidence_score: float = Field(ge=0.0, le=1.0, description="How well this query was answered (0-1)")
    documentation_gaps: List[str] = Field(description="Missing or unclear documentation areas")
    insight_categories: Dict[str, List[str]] = Field(description="Categorized insights by type")
```

### Confidence Scoring Algorithm

```python
# LLM-based scoring with structured output validation
confidence_score = structured_analysis.confidence_score  # 0.0-1.0 from LLM analysis

# Quality assessment criteria:
# 1.0: Comprehensive documentation with clear examples and explanations
# 0.8: Good documentation with some examples
# 0.6: Basic documentation, limited examples
# 0.4: Minimal documentation, unclear explanations
# 0.2: Very poor documentation, confusing or missing key info
# 0.0: No relevant documentation found

# Fallback scoring when LLM fails
if not structured_analysis:
    confidence_score = min(1.0, len(results) / 8.0)  # Base score from result count
    if has_code_examples:
        confidence_score += 0.2
```

## Stage 2: Evidence-Based Plan Generation

### Objective
Generate 6 diverse project plans that test different aspects of agent capabilities, grounded in actual documentation quality.

### Plan Categories

#### 1. Beginner-Friendly Implementation
- **Focus**: Basic setup and simple usage
- **Tests**: Agent's ability to follow setup instructions and implement basic functionality
- **Objectives**: Set up tool, perform basic operations, handle simple success cases

#### 2. Authentication & Security
- **Focus**: Authentication, API keys, security patterns
- **Tests**: Agent's ability to handle security configurations and auth flows
- **Objectives**: Configure authentication, handle auth errors, secure API usage

#### 3. Integration Workflow
- **Focus**: Integration with other systems and services
- **Tests**: Agent's ability to manage dependencies and data flow
- **Objectives**: Integrate with external services, handle data flow, manage dependencies

#### 4. Error Handling & Edge Cases
- **Focus**: Error scenarios and edge case management
- **Tests**: Agent's robustness in handling failures and unexpected situations
- **Objectives**: Handle common errors, manage edge cases, implement retries

#### 5. Performance & Optimization
- **Focus**: Performance, optimization, and scaling
- **Tests**: Agent's ability to implement efficient solutions
- **Objectives**: Optimize performance, handle large datasets, scale efficiently

#### 6. Advanced Features
- **Focus**: Advanced capabilities and customization
- **Tests**: Agent's ability to use complex features and extend functionality
- **Objectives**: Use advanced features, customize behavior, extend functionality

### Challenge Prediction

For each plan category, the agent predicts likely implementation challenges based on:
- **Category-specific patterns**: Known common issues for each category type
- **Documentation gaps**: Issues identified during exploration phase
- **Confidence scores**: Lower confidence areas suggest higher difficulty

## Stage 3: Targeted Refinement

### Objective
Strengthen each plan with specific supporting evidence and accurate difficulty assessment.

### Refinement Process

For each generated plan:

1. **Generate Specific Queries**: Create targeted queries for this plan's focus area
   ```python
   refinement_queries = [
       f"{tool_name} {plan.category} specific examples implementation",
       f"{tool_name} {plan.category} tutorial step by step", 
       f"{tool_name} {plan.category} common issues errors troubleshooting",
       f"{tool_name} {plan.category} best practices patterns"
   ]
   ```

2. **Gather Supporting Evidence**: Retrieve documentation for each query (k=5 per query)

3. **Calculate Evidence Strength**: Score plan supportiveness (0.0-1.0) based on:
   - Number of relevant documents found
   - Presence of code examples
   - Quality and specificity of content

4. **Update Difficulty Assessment**:
   - **Easy** (evidence_strength > 0.7): Well-documented with examples
   - **Medium** (evidence_strength > 0.4): Partially documented
   - **Hard** (evidence_strength ≤ 0.4): Poorly documented or complex

5. **Extract Implementation Details**:
   - Key APIs and features mentioned
   - Specific documentation references
   - Prerequisites and dependencies

### Evidence Strength Calculation

```python
# Base evidence quality per query
evidence_quality = len(results) / 5.0  # Normalize to 0-1

# Bonus for code examples
if has_code_examples:
    evidence_quality += 0.2

# Overall plan evidence strength
plan.evidence_strength = total_evidence_quality / num_queries
```

## Project Scratchpad

### Purpose
The scratchpad accumulates and organizes knowledge throughout the exploration process, serving as the knowledge base for plan generation.

### Data Structure

```python
@dataclass
class ProjectScratchpad:
    tool_name: str
    exploration_rounds: List[ExplorationRound]
    
    # Accumulated insights by category
    core_capabilities: List[str]
    common_patterns: List[str] 
    setup_requirements: List[str]
    authentication_info: List[str]
    integration_patterns: List[str]
    advanced_features: List[str]
    
    # Quality assessment
    well_documented_areas: List[str]
    documentation_gaps: List[str]
    confidence_areas: List[str]
    uncertain_areas: List[str]
```

### Knowledge Accumulation

As each exploration round completes, insights are automatically categorized and accumulated using structured output:

#### Structured Categorization
- **Core capabilities**: From `core_features` category in structured analysis
- **Setup requirements**: From `setup` category in structured analysis
- **Authentication info**: From `authentication` category in structured analysis
- **Advanced features**: From `advanced` category in structured analysis
- **Integration patterns**: From `integration` category in structured analysis
- **Common patterns**: From `patterns` category in structured analysis
- **Quality assessment**: From confidence scores and documentation gaps

#### Improved Categorization Logic
```python
def _update_cumulative_insights(self, round_data: ExplorationRound):
    # Use structured analysis if available
    if round_data.structured_analysis:
        categories = round_data.structured_analysis.insight_categories
        
        # Map structured categories to scratchpad fields
        self.core_capabilities.extend(categories.get("core_features", []))
        self.setup_requirements.extend(categories.get("setup", []))
        self.authentication_info.extend(categories.get("authentication", []))
        self.advanced_features.extend(categories.get("advanced", []))
        self.integration_patterns.extend(categories.get("integration", []))
        self.common_patterns.extend(categories.get("patterns", []))
    else:
        # Enhanced fallback categorization with better keyword matching
        # ...
```

#### Benefits of Structured Categorization
- **Eliminates empty fields**: Previously empty categories like `documentation_gaps` are now properly populated
- **Reliable parsing**: LangChain structured output ensures consistent JSON parsing  
- **Better distribution**: Insights are distributed across appropriate categories instead of dumping everything into `common_patterns`
- **Fallback handling**: Enhanced keyword-based categorization when LLM analysis fails

## Output Format

### Project Plans
Each generated plan includes:

```python
@dataclass  
class ProjectPlan:
    plan_id: str                    # Unique identifier
    title: str                      # Human-readable title
    category: str                   # Plan category type
    description: str                # What this plan tests
    main_objectives: List[str]      # What agent should accomplish
    success_criteria: List[str]     # How to measure success
    expected_challenges: List[str]  # Predicted difficulties
    
    # Evidence and quality metrics
    supporting_queries: List[str]   # Queries used for refinement
    evidence_strength: float        # Documentation quality score
    documentation_refs: List[str]   # Specific doc references
    estimated_difficulty: str       # easy/medium/hard
    prerequisite_knowledge: List[str]
    key_apis_or_features: List[str]
```

### Exploration Summary
Complete exploration data including:
- All exploration rounds with queries and results
- Cumulative understanding summary
- Documentation quality assessment
- Identified gaps and strong areas

## Usage Example

```python
from progressive_planner import ProgressivePlannerAgent

# Initialize with vector store
planner = ProgressivePlannerAgent(vector_store_manager)

# Generate plans for a tool
plans, scratchpad = planner.plan_project("FastAPI")

# Access results
for plan in plans:
    print(f"Plan: {plan.title}")
    print(f"Difficulty: {plan.estimated_difficulty}")
    print(f"Evidence strength: {plan.evidence_strength:.2f}")
    print(f"Objectives: {plan.main_objectives}")
```

## Key Benefits

### 1. Grounded in Reality
Plans are based on actual documentation content, not assumptions about what should be documented.

### 2. Quality Awareness
Each plan comes with evidence strength scores, helping prioritize testing efforts.

### 3. Gap Identification
Explicitly identifies documentation weaknesses, enabling targeted improvements.

### 4. Diverse Coverage
Six different plan categories ensure comprehensive testing of agent capabilities.

### 5. Evidence Trail
Complete trail of queries and evidence supporting each plan decision.

### 6. Adaptive Difficulty
Difficulty assessment adapts based on actual documentation quality, not generic assumptions.

### 7. Structured Output Reliability
LangChain structured output ensures consistent parsing and eliminates empty exploration data fields.

### 8. Improved Categorization
Insights are properly distributed across categories instead of being dumped into generic buckets, providing richer analysis.

## Integration with RAG-Enhanced Coding Agent

The progressive planner's output directly feeds into the RAG-Enhanced Coding Agent:

1. **Plan Selection**: Coding agent selects plans based on difficulty and evidence strength
2. **Context Awareness**: Uses scratchpad insights to build better RAG queries
3. **Gap Handling**: Knows which areas have poor documentation, adjusting expectations
4. **Evidence Utilization**: Leverages specific documentation references for implementation

This creates a cohesive system where planning intelligence directly improves coding performance.

## Future Enhancements

### Dynamic Query Generation
- Adaptive query generation based on tool characteristics
- Learning from previous planning sessions
- Tool-specific query templates

### Cross-Tool Learning
- Pattern recognition across multiple tools
- Reusable insights for similar tool categories
- Comparative analysis capabilities

### Continuous Improvement
- Feedback loop from coding agent results
- Plan effectiveness scoring
- Self-improving query strategies

### Enhanced Structured Output
- Additional insight categories for specialized domains
- Confidence calibration across different LLM models
- Multi-modal analysis for documentation with images/diagrams
- Automatic schema adaptation based on tool types

## Conclusion

The Progressive Planner Agent represents a significant advancement in AI-driven project planning by:
- **Learning before planning** through systematic exploration
- **Grounding plans in evidence** rather than assumptions
- **Providing quality assessment** alongside plan generation
- **Identifying documentation gaps** for improvement
- **Using structured output** for reliable and consistent analysis
- **Proper insight categorization** that eliminates empty exploration data fields

This approach results in more realistic, achievable, and useful project plans that directly improve agent readiness evaluation outcomes. The recent integration of LangChain structured output specifically addresses previous issues with incomplete exploration data, ensuring all categories are properly populated and insights are accurately categorized.